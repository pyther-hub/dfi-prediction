{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstatboffin\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/sudhanshurai/.netrc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import timm\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "wandb.login(key='0f806ca646c884d0d0105d882367d470d2d664e4')\n",
    "\n",
    "DATASET_PTH = '/Users/sudhanshurai/Desktop/Dilip sir/dfi prediction/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "\tdef __init__(self, img_dir, df, transform=None):\n",
    "\t\tself.img_dir = img_dir\n",
    "\t\tself.img_name = df['file_name'].values\n",
    "\t\tself.img_labels = df['label'].values.astype(float)\n",
    "\t\tself.transform = transform\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.img_name)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timg_path = os.path.join(self.img_dir, self.img_name[idx])\n",
    "\t\timage = Image.open(img_path).convert(\"RGB\")\n",
    "\t\tlabel = self.img_labels[idx]\n",
    "\t\tif self.transform:\n",
    "\t\t\timage = self.transform(image)\n",
    "\t\treturn image, label\n",
    "\t\n",
    "def get_augmentations(phase):\n",
    "\tif phase == 'train':\n",
    "\t\treturn transforms.Compose([\n",
    "\t\t\ttransforms.Resize((224,224)),\n",
    "\t\t\ttransforms.RandomHorizontalFlip(),\n",
    "\t\t\ttransforms.RandomVerticalFlip(),\n",
    "\t\t\ttransforms.RandomRotation(90),\n",
    "\t\t\ttransforms.ToTensor(),\n",
    "\t\t\ttransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ,\n",
    "\n",
    "\t\t])\n",
    "\telse:\n",
    "\t\treturn transforms.Compose([\n",
    "\t\t\ttransforms.Resize((224,224)),\n",
    "\t\t\ttransforms.ToTensor(),\n",
    "\t\t\ttransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ,\n",
    "\t\t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_dataset(val_numb: int):\n",
    "\timg_dir = DATASET_PTH\n",
    "\ttrain_transform = get_augmentations('train')\n",
    "\ttest_transform = get_augmentations('test')\n",
    "\ttrain_val_df = pd.read_csv(os.path.join(img_dir, 'annotations.csv'))\n",
    "\t\n",
    "\ttrain_df = train_val_df[train_val_df['donnor'] != val_numb]\n",
    "\ttest_df = train_val_df[train_val_df['donnor'] == val_numb]\n",
    "\t\n",
    "\ttrain_dataset = CustomImageDataset(img_dir, train_df, train_transform)\n",
    "\ttest_dataset = CustomImageDataset(img_dir, test_df, test_transform)\n",
    "\n",
    "\treturn train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import timm\n",
    "\n",
    "# Function to create SEResNext model with enhanced classifier and dropout\n",
    "def create_seresnext_model(dropout_prob=0.5):\n",
    "\tmodel = timm.create_model('seresnext50_32x4d', pretrained=True)\n",
    "\tnum_ftrs = model.fc.in_features\n",
    "\tmodel.fc = torch.nn.Sequential(\n",
    "\t\ttorch.nn.Linear(num_ftrs, 512),\n",
    "\t\ttorch.nn.ReLU(),\n",
    "\t\ttorch.nn.Dropout(dropout_prob),  # Dropout layer added\n",
    "\t\ttorch.nn.Linear(512, 256),\n",
    "\t\ttorch.nn.ReLU(),\n",
    "\t\ttorch.nn.Dropout(dropout_prob),  # Dropout layer added\n",
    "\t\ttorch.nn.Linear(256, 1)  # Adjust for regression task\n",
    "\t)\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_metrics(labels, outputs):\n",
    "\tmean_absolute_error_value = mean_absolute_error(labels, outputs)\n",
    "\tmean_squared_error_value = mean_squared_error(labels, outputs)\n",
    "\tpearson_correlation, _ = pearsonr(labels, outputs)  # Use pearsonr to calculate Pearson correlation\n",
    "\t\n",
    "\treturn mean_absolute_error_value, mean_squared_error_value, pearson_correlation\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, criterion, optimizer, logger=None, num_epochs=25):\n",
    "\t\n",
    "\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\tmodel.to(device)\n",
    "\t\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tmodel.train()  # Set model to training mode\n",
    "\t\ttrain_running_loss = 0.0\n",
    "\t\ttrain_all_labels = []\n",
    "\t\ttrain_all_outputs = []\n",
    "\t\t\n",
    "\t\tfor train_inputs, train_labels in tqdm(train_dataloader, desc=\"Training\"):\n",
    "\t\t\ttrain_inputs = train_inputs.to(device)\n",
    "\t\t\ttrain_labels = train_labels.to(device)\n",
    "\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\ttrain_outputs = model(train_inputs)\n",
    "\t\t\ttrain_loss = criterion(train_outputs.squeeze(), train_labels.float())\n",
    "\t\t\t\t\t\t\n",
    "\t\t\ttrain_loss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\ttrain_running_loss += train_loss.item()\n",
    "\t\t\ttrain_all_labels.append(train_labels.cpu().numpy())\n",
    "\t\t\ttrain_all_outputs.append(train_outputs.squeeze().cpu().detach().numpy())\n",
    "\t\t\n",
    "\t\ttrain_epoch_loss = train_running_loss / len(train_dataloader)\n",
    "\t\ttrain_all_labels = np.concatenate(train_all_labels)\n",
    "\t\ttrain_all_outputs = np.concatenate(train_all_outputs)\n",
    "\n",
    "\t\ttrain_mean_absolute_error, train_mean_squared_error, train_pearson_correlation = calculate_metrics(train_all_labels, train_all_outputs)\n",
    "\n",
    "\t\tprint(f'Training Loss: {train_epoch_loss:.4f}')\n",
    "\t\tprint(f'Training Pearson Correlation: {train_pearson_correlation:.4f}')\n",
    "\n",
    "\t\t# Validation phase\n",
    "\t\tval_running_loss = 0.0\n",
    "\t\tval_all_labels = []\n",
    "\t\tval_all_outputs = []\n",
    "\t\tmodel.eval()  # Set model to evaluate mode\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor val_inputs, val_labels in tqdm(val_dataloader, desc=\"Validation\"):\n",
    "\t\t\t\tval_inputs = val_inputs.to(device)\n",
    "\t\t\t\tval_labels = val_labels.to(device)\n",
    "\t\t\t\t# Forward pass\n",
    "\t\t\t\tval_outputs = model(val_inputs)\n",
    "\t\t\t\tval_loss = criterion(val_outputs.squeeze(), val_labels.float())\n",
    "\n",
    "\t\t\t\t# Statistics\n",
    "\t\t\t\tval_running_loss += val_loss.item()\n",
    "\t\t\t\tval_all_labels.append(val_labels.cpu().numpy())\n",
    "\t\t\t\tval_all_outputs.append(val_outputs.squeeze().cpu().numpy())\n",
    "\t\t\n",
    "\t\tval_epoch_loss = val_running_loss / len(val_dataloader)\n",
    "\t\tval_all_labels = np.concatenate(val_all_labels)\n",
    "\t\tval_all_outputs = np.concatenate(val_all_outputs)\n",
    "\n",
    "\t\t# Calculate validation metrics\n",
    "\t\tval_mean_absolute_error, val_mean_squared_error, val_pearson_correlation = calculate_metrics(val_all_labels, val_all_outputs)\n",
    "\n",
    "\t\t# Logging\n",
    "\t\tif logger is not None:\n",
    "\t\t\tlogger.log({\n",
    "\t\t\t\t'train_loss': train_epoch_loss,\n",
    "\t\t\t\t'val_loss': val_epoch_loss,\n",
    "\t\t\t\t'val_mean_absolute_error': val_mean_absolute_error,\n",
    "\t\t\t\t'val_mean_squared_error': val_mean_squared_error,\n",
    "\t\t\t\t'val_pearson_correlation': val_pearson_correlation,\n",
    "\t\t\t\t'epoch_number': epoch + 1  # Epoch number starting from 1\n",
    "\t\t\t})\n",
    "\n",
    "\t\tprint(f'Validation Loss: {val_epoch_loss:.4f}')\n",
    "\t\tprint(f'Validation Pearson Correlation: {val_pearson_correlation:.4f}')\n",
    "\n",
    "\tprint('Training and Validation complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunExperiment(set_numb, model_name, numb_epochs, lr=0.0001):\n",
    "\tmodel_dict = {\n",
    "\t\t'seresnext': create_seresnext_model,\n",
    "\t\t# 'resnet': create_resnet_model,\n",
    "\t\t# 'inception': create_inception_model,\n",
    "\t\t# 'densenet': create_densenet_model,\n",
    "\t\t# 'efficientnet': create_efficientnet_model,\n",
    "\t\t# 'vit_base_patch16_224': create_vit_model,\n",
    "\t\t# 'mobilenetv3large': create_mobilenetv3large_model,\n",
    "\t\t# 'vgg19': create_vgg19_model\n",
    "\t}\n",
    "\n",
    "\t# Create model\n",
    "\tmodel = model_dict[model_name](0.0001)\n",
    "\t\n",
    "\t# Load datasets\n",
    "\ttrain_dataset, val_dataset = get_train_test_dataset(set_numb)\n",
    "\t\n",
    "\t# Create data loaders\n",
    "\ttrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=1)\n",
    "\tval_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=1)\n",
    "\t\n",
    "\t# Define loss function and optimizer\n",
    "\tcriterion = nn.MSELoss()\n",
    "\toptimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\t\n",
    "\trun = wandb.init(\n",
    "\t\tproject=\"DFI_prediction\",\n",
    "\t\tname=f\"tp_{model_name}_main_kaggle_set_numb_{set_numb}\",\n",
    "\t\tconfig={\n",
    "\t\t\t'model_name': model_name,\n",
    "\t\t\t'num_epochs': numb_epochs,\n",
    "\t\t\t'learning_rate': lr,\n",
    "\t\t\t'batch_size': 32,\n",
    "\t\t\t'dataset_number': set_numb\n",
    "\t\t}\n",
    "\t)\n",
    "\t\n",
    "\t# Train and test the model\n",
    "\ttrain_model(model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs=numb_epochs, logger=run)\n",
    "\t\n",
    "\t\n",
    "\t# Finish the WandB run\n",
    "\tif run is not None:\n",
    "\t\trun.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:npha9pyr) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Experiment_seresnext_main_kaggle_set_numb_2</strong> at: <a href='https://wandb.ai/statboffin/DFI_prediction/runs/npha9pyr' target=\"_blank\">https://wandb.ai/statboffin/DFI_prediction/runs/npha9pyr</a><br/> View project at: <a href='https://wandb.ai/statboffin/DFI_prediction' target=\"_blank\">https://wandb.ai/statboffin/DFI_prediction</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240714_181529-npha9pyr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:npha9pyr). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/sudhanshurai/Desktop/Dilip sir/dfi prediction/wandb/run-20240714_181640-dvhtwc4g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/statboffin/DFI_prediction/runs/dvhtwc4g' target=\"_blank\">tp_seresnext_main_kaggle_set_numb_2</a></strong> to <a href='https://wandb.ai/statboffin/DFI_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/statboffin/DFI_prediction' target=\"_blank\">https://wandb.ai/statboffin/DFI_prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/statboffin/DFI_prediction/runs/dvhtwc4g' target=\"_blank\">https://wandb.ai/statboffin/DFI_prediction/runs/dvhtwc4g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/29 [00:00<?, ?it/s]\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/sudhanshurai/.pyenv/versions/3.9.10/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/sudhanshurai/.pyenv/versions/3.9.10/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "  File \"/Users/sudhanshurai/Desktop/Dilip sir/dfi prediction/myenv/lib/python3.9/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/sudhanshurai/Desktop/Dilip sir/dfi prediction/myenv/lib/python3.9/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/sudhanshurai/Desktop/Dilip sir/dfi prediction/myenv/lib/python3.9/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/sudhanshurai/Desktop/Dilip sir/dfi prediction/myenv/lib/python3.9/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/sudhanshurai/Desktop/Dilip sir/dfi prediction/myenv/lib/python3.9/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/sudhanshurai/Desktop/Dilip sir/dfi prediction/myenv/lib/python3.9/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/sudhanshurai/.pyenv/versions/3.9.10/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/sudhanshurai/.pyenv/versions/3.9.10/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'CustomImageDataset' on <module '__main__' (built-in)>\n",
      "Training:   0%|          | 0/29 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 6592) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/Dilip sir/dfi prediction/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/lib/python3.9/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/lib/python3.9/multiprocessing/connection.py:262\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/lib/python3.9/multiprocessing/connection.py:429\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 429\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/lib/python3.9/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/Dilip sir/dfi prediction/myenv/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 6592) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mRunExperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mseresnext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 40\u001b[0m, in \u001b[0;36mRunExperiment\u001b[0;34m(set_numb, model_name, numb_epochs, lr)\u001b[0m\n\u001b[1;32m     27\u001b[0m run \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[1;32m     28\u001b[0m \tproject\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDFI_prediction\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m \tname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtp_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_main_kaggle_set_numb_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mset_numb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \t}\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Train and test the model\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumb_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Finish the WandB run\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[27], line 25\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, criterion, optimizer, logger, num_epochs)\u001b[0m\n\u001b[1;32m     22\u001b[0m train_all_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     23\u001b[0m train_all_outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_inputs, train_labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     26\u001b[0m \ttrain_inputs \u001b[38;5;241m=\u001b[39m train_inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     27\u001b[0m \ttrain_labels \u001b[38;5;241m=\u001b[39m train_labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Desktop/Dilip sir/dfi prediction/myenv/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Dilip sir/dfi prediction/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Desktop/Dilip sir/dfi prediction/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Dilip sir/dfi prediction/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Desktop/Dilip sir/dfi prediction/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1145\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 6592) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "RunExperiment(2, 'seresnext', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
